[toc]



# 第10章 数据聚合与分组运算（Data Aggregation and Group Operations）

对数据集进⾏分组并对各组应⽤⼀个函数（⽆论是聚合还是转换）。通常就是计算分组统计或⽣成透视表。pandas提供了⼀个灵活⾼效的**gruopby**功能，它使你能以⼀种自然的⽅式对数据集进⾏切⽚、切块、摘要等操作。

* 计算分组摘要统计，如计数、平均值、标准差，或⽤户⾃定义函数。
* 计算分组的概述统计，⽐如数量、平均值或标准差，或是⽤户定义的函数。
* 应⽤组内转换或其他运算，如规格化、线性回归、排名或选取子集等。
* 计算透视表或交叉表。
* 执行分位数分析以及其它统计分组分析。

## 10.1 groupby机制（GroupBy Mechanics）

#### 分组聚合演示（表示分组运算）

<img src="F:\github_desktop\github文件路径\books\python-for-data-analysis_2nd\chapter 10 pictures\image-20201012145416486.png" alt="image-20201012145416486" style="zoom:50%;" />

`groupby()`

~~~python
df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                   'key2' : ['one', 'two', 'one', 'two', 'one'],
                   'data1' : np.random.randn(5),
                   'data2' : np.random.randn(5)})
#按照某些列的数据进行提取组合，实现求平均值
means = df['data1'].groupby([df['key1'], df['key2']]).mean()
df.groupby(['key1', 'key2']).size()
~~~

~~~python
#Episode
#利用groupby做分组统计
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
huobo_19=pd.read_excel('./19融资信息.xls')
oof=pd.Series(huobo_19.groupby(['公司']).size(),name='融资情况')
~~~



:spider_web:拿groupby做什么，都有可能会⽤到GroupBy的size⽅法，它可以返回⼀个含有分组⼤⼩的**Series**

#### 对分组进行迭代（Iterating Over Groups）

grouped = df['data1'].groupby(df['key1'])，其中grouped是一个GroupBy对象，它**支持迭代**。

~~~python
df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                   'key2' : ['one', 'two', 'one', 'two', 'one'],
                   'data1' : np.random.randn(5),
                   'data2' : np.random.randn(5)})
#oof=DataFrame(np.random.randn(2,3),index=pd.Index(['a','b']),columns=pd.Index(['one','two','three']))
for name, group in df.groupby('key1'):
    print(name)
    print(group)
#以某一列进行分组
pieces = dict(list(df.groupby('key1')))
pieces['b']
df.dtypes
#以某数值类型进行分组
grouped = df.groupby(df.dtypes, axis=1)
~~~

#### 选取一列或列的子集（Selecting a Column or Subset of Columns）

DataFrame产⽣的GroupBy对象，如果⽤⼀个（单个字符串）或⼀组（字符串数组）列名对其进⾏索引，就能实现选取部分列进⾏聚合的⽬的（参看上一例）。

对于大数据集来说，往往只需要对部分列进行聚合。

:spider_web:除了groupby之外，还有merge。两者可以作比较。（主要区别：merge直接显示，groupby不能）

~~~python
#取数据集中的key1，key2中的作为分组依据，对data2中的数据进行求平均值
#df.groupby(['key1', 'key2'])[['data2']].mean()
~~~

#### 通过字典或Series进行分组（Grouping with Dicts and Series）

定义字典来进行统计分组计算

~~~python
people = pd.DataFrame(np.random.randn(5, 5),
                      columns=['a', 'b', 'c', 'd', 'e'],
                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values
#通过字典来进行分组
mapping = {'a': 'red', 'b': 'red', 'c': 'blue',
           'd': 'blue', 'e': 'red', 'f' : 'orange'}
by_column = people.groupby(mapping, axis=1)
by_column.sum()
#通过Series来进行分组
map_series = pd.Series(mapping)
people.groupby(map_series, axis=1).count()
#上述操作mapping这种对应适用于小数据集，如何将他适用范围变大
#以前的知识（映射map）
mapp={'Joe':'re', 'Steve':'bl', 'Wes':'re', 'Jim':'bl', 'Travis':'re'}
people['新加一列']=people.index.map(mapp)
~~~

:spider_web:上述的sum()和count()函数的用法类似

#### 通过函数进行分组（Grouping with Functions）

~~~python
people = pd.DataFrame(np.random.randn(5, 5),
                      columns=['a', 'b', 'c', 'd', 'e'],
                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values
key_list = ['one', 'one', 'one', 'two', 'two']
people.groupby([len, key_list]).min()
~~~

#### 根据索引级别分组（Grouping by Index Levels）

根据轴索引进行聚合

~~~python
#多表格样式MultiIndex
columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
                                    [1, 3, 5, 1, 3]],
                                    names=['cty', 'tenor'])
hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)
#hier_df.to_excel('./zhi.xlsx')
hier_df.groupby(level='cty', axis=1).count()
~~~

## 10.2 数据聚合（Data Aggregation）

聚合指的是任何能够从数组产⽣标量值的数据转换过程。

**quantile**可以计算Series或DataFrame列的样本分位数。

~~~python
df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                   'key2' : ['one', 'two', 'one', 'two', 'one'],
                   'data1' : np.random.randn(5),
                   'data2' : np.random.randn(5)})
grouped = df.groupby('key1')
grouped['data1'].quantile(0.9)

def peak_to_peak(arr):
    return arr.max() - arr.min()
#使⽤你⾃⼰的聚合函数，只需将其传⼊aggregate或agg⽅法(不建议使用)
grouped.agg(peak_to_peak)
~~~

#### 面向列的多函数应用（Column-Wise and Multiple Function Application）

小费数据集

~~~python
tips = pd.read_csv('examples/tips.csv')
# Add tip percentage of total bill
tips['tip_pct'] = tips['tip'] / tips['total_bill']
tips[:6]
#以day和smoker为分组依据，对tip_pct做出聚合操作（mean）
grouped = tips.groupby(['day', 'smoker'])
grouped_pct = grouped['tip_pct']
grouped_pct.agg('mean')

def peak_to_peak(arr):
    return arr.max() - arr.min()
grouped_pct.agg(['mean', 'std', peak_to_peak])

grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])

#横纵列都是多索引，对tip_pct和total_bill列计算三个统计信息（count，mean，max）
functions = ['count', 'mean', 'max']
result = grouped['tip_pct', 'total_bill'].agg(functions)
result

#传⼊带有⾃定义名称的⼀组元组
ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]
grouped['tip_pct', 'total_bill'].agg(ftuples)

#对⼀个列或不同的列应⽤不同的函数。具体的办法是向agg传⼊⼀个从列名映射到函数的字典
grouped.agg({'tip' : np.max, 'size' : 'sum'})
grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],
             'size' : 'sum'})
~~~

#### 以“没有行索引”的形式返回聚合数据（Returning Aggregated Data Without Row Indexes）

所有示例中的聚合数据都有由唯⼀的分组键组成的索引（可能还是层次化的）。大多数情况下，我们可以**向groupby中传入as_index=False**

~~~python
tips.groupby(['day', 'smoker'], as_index=False).mean()
#下面和上面的方法的比较，就可以突出上面这一方法的好处，这个太重要了
ooff=tips.groupby(['day', 'smoker']).mean()
ooff.unstack()
~~~

## 10.3 “拆分－应用－合并”（Apply: General split-apply-combine）

最通⽤的GroupBy⽅法是apply，本节剩余部分将重点讲解它。如图所示，apply会将待处理的对象拆分成多个⽚段，然后对各⽚段调⽤传⼊的函数，最后尝试将各⽚段组合到⼀起。

<img src="F:\github_desktop\github文件路径\books\python-for-data-analysis_2nd\chapter 10 pictures\image-20201012145416486.png" alt="image-20201012145416486" style="zoom:50%;" />

~~~python
#根据分组得到最高的5个tip_pct值
tips = pd.read_csv('examples/tips.csv')
# Add tip percentage of total bill
tips['tip_pct'] = tips['tip'] / tips['total_bill']
tips[:6]
def top(df, n=5, column='tip_pct'):
    return df.sort_values(by=column)[-n:]
#method1 ⼀个层次化索引，其内层索引值来⾃原DataFrame
top(tips, n=6)
#method2 传给apply的函数能够接受其他参数或关键字，则可以将这些内容放在函数名后⾯⼀并传⼊
tips.groupby('smoker').apply(top)

#下面发挥groupby和apply的强大之处
tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')


result = tips.groupby('smoker')['tip_pct'].describe()
result.unstack('smoker')

#使用lambda中的describe()方法构造函数。在进行应用apply
f = lambda x: x.describe()
tips.groupby('smoker').apply(f)
~~~

:spider_web:笔记：除这些基本用法之外，能否充分发挥apply的威力很大程度上取决于你的创造力。传⼊的那个函数能做什么全由你说了算，它只需返回⼀个pandas对象或标量值即可。

#### 禁止分组键（Suppressing the Group Keys）

分组键会跟原始对象的索引共同构成结果对象中的层次化索引。将group_keys=False传⼊groupby即可禁止该效果。

:spider_web:是不是跟groupby中的as_index=False类似

~~~python
#比较二者的区别
tips.groupby('smoker', group_keys=False).apply(top)
tips.groupby(['day', 'smoker'], as_index=False).mean()
~~~

:smile_cat:禁止分组键和以“没有行索引”的形式返回聚合数据的两种方法都可以采用

#### 分位数和桶分析（Quantile and Bucket Analysis）

这里就要用到第八章中的cut和qcut方法。将这些函数跟groupby结合起来，就能⾮常轻松地实现对数据集的桶（bucket）或分位数（quantile）分析。

~~~python
frame = pd.DataFrame({'data1': np.random.randn(1000),
                      'data2': np.random.randn(1000)})
quartiles = pd.cut(frame.data1, 4)
quartiles[:10]
#bucket analysis
def get_stats(group):
    return {'min': group.min(), 'max': group.max(),
            'count': group.count(), 'mean': group.mean()}
#下面这一步我还是不太理解
#思路：groupby只是分组依据，实际上的apply是frame.data2
oth2_quartiles = pd.cut(frame.data2, 4)
grouped = frame.data2.groupby(oth2_quartiles)
grouped.apply(get_stats).unstack()
# Return quantile numbers
grouping = pd.qcut(frame.data1, 10, labels=False)
grouped = frame.data2.groupby(grouping)
grouped.apply(get_stats).unstack()
~~~

:spider_web:cut和value_counts()一般连用来实现区间统计操作

:spider_web:而groupby和cut连用用来实现区间分组，然后再使用其他操作

:spider:根据样本分位数得到大小相等的桶（数量，count值），使⽤qcut即可。传入labels=False即可只获取分位数的编号。

:spider_web:在第12章详细讲解pandas的**Categorical类型（cut所返回）**

#### 示例：用特定于分组的值填充缺失值（Example: Filling Missing Values with Group-Specific Values）

对于缺失数据的清理工作，第一时间想到的就是dropna（直接删除掉），当然还有填充（fillna）另外还有其他的方法比如插值法等。

下面的小案例是使用分组中的平均值取填充NA值

~~~python
states = ['Ohio', 'New York', 'Vermont', 'Florida',
          'Oregon', 'Nevada', 'California', 'Idaho']
group_key = ['East'] * 4 + ['West'] * 4
data = pd.Series(np.random.randn(8), index=states)
data[['Vermont', 'Nevada', 'Idaho']] = np.nan

fill_mean = lambda g: g.fillna(g.mean())
data.groupby(group_key).apply(fill_mean)

#由于分组具有⼀个name属性，所以我们可以拿来⽤⼀下
fill_values = {'East': 0.5, 'West': -1}
fill_func = lambda g: g.fillna(fill_values[g.name])
data.groupby(group_key).apply(fill_func)
~~~

#### 示例：随机采样和排列（Example: Random Sampling and Permutation）

假设你想要从⼀个⼤数据集中随机抽取（进⾏替换或不替换）样本以进⾏蒙特卡罗模拟（Monte Carlo simulation）或其他分析工作。

下面我们来玩牌:smiley_cat:

~~~python
#Hearts, Spades, Clubs, Diamonds card type
#造牌操作
suits = ['H', 'S', 'C', 'D']
card_val = (list(range(1, 11)) + [10] * 3) * 4
base_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']
cards = []
for suit in ['H', 'S', 'C', 'D']:
    cards.extend(str(num) + suit for num in base_names)
deck = pd.Series(card_val, index=cards)
deck[:13]
#定义随机抽取5张牌的操作
def draw(deck, n=5):
    return deck.sample(n)
draw(deck)

#下面我想要在hearts spades clubs diamond四种suit中各随机抽取2张牌
get_suit = lambda card: card[-1] # last letter is suit
deck.groupby(get_suit).apply(draw, n=2)

deck.groupby(get_suit,group_keys=False).apply(draw, n=2)
#此处就不能向groupby中传入参数as_index=False
~~~

#### 示例：分组加权平均数和相关系数(Example: Group Weighted Average and Correlation)

根据groupby的“拆分－应⽤－合并(split-apply-combine)”范式，可以进⾏DataFrame的列与列之间或两个Series之间的运算（⽐如分组加权平均）。以下⾯这个数据集为例，它含有分组键、值以及⼀些权重值。

~~~python
df = pd.DataFrame({'category': ['a', 'a', 'a', 'a',
                                'b', 'b', 'b', 'b'],
                   'data': np.random.randn(8),
                   'weights': np.random.rand(8)})
#scipy中加权平均值
grouped = df.groupby('category')
get_wavg = lambda g: np.average(g['data'], weights=g['weights'])
grouped.apply(get_wavg)
~~~

来⾃Yahoo_Finance的数据集，其中含有几只股票和**标准普尔500指数**（符号SPX）的收盘价(想学金融的也要好好python哦),这个要在看一下:arrow_down_small:

~~~python
#文档小案例
close_px = pd.read_csv('examples/stock_px_2.csv', parse_dates=True,
                       index_col=0)
#基本信息，可以帮助我们知道csv中每一列中是否有缺失
close_px.info()
close_px[-4:]
#任务：计算⼀个由⽇收益率（通过百分数变化计算）与SPX之间的年度相关系数组成的DataFrame。
#:spider_web:**pandas.DataFrame.corrwith**用于计算DataFrame中行与行或者列与列之间的相关性
spx_corr = lambda x: x.corrwith(x['SPX'])
#使⽤pct_change计算close_px的百分⽐变化
rets = close_px.pct_change().dropna()

get_year = lambda x: x.year
by_year = rets.groupby(get_year)
by_year.apply(spx_corr)

by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))
~~~

:spider_web:**pandas.DataFrame.corrwith**用于计算DataFrame中行与行或者列与列之间的相关性

:spider_web:使⽤[pct_change](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pct_change.html)计算close_px的百分比变化

:spider_web:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pct_change.html

#### 示例：组级别的线性回归(Example: Group-Wise Linear Regression)

⽤groupby执⾏更为复杂的分组统计分析，只要函数返回的是pandas对象或标量值即可。

定义下⾯这个regress函数（利⽤statsmodels计量经济学库）对各数据块执⾏普通最⼩⼆乘法（Ordinary Least Squares，OLS）回归。

~~~python
import statsmodels.api as sm
def regress(data, yvar, xvars):
    Y = data[yvar]
    X = data[xvars]
    X['intercept'] = 1.
    result = sm.OLS(Y, X).fit()
    return result.params

by_year.apply(regress, 'AAPL', ['SPX'])
~~~

:spider_web:没有包statsmodel

## 10.4 透视表和交叉表

#### 透视表（pivot table）

透视表（pivot table）是各种电⼦表格程序和其他数据分析软件中⼀种常见的数据汇总⼯具。

它根据⼀个或多个键对数据进⾏聚合，并根据⾏和列上的分组键将数据分配到各个矩形区域中。

Python和pandas中，可以通过本章所介绍的groupby功能以及（能够利⽤层次化索引的）重塑运算制作透视表。DataFrame有⼀个pivot_table⽅法，此外还有⼀个顶级的**pandas.pivot_table**函
数。除能为groupby提供便利之外，pivot_table还可以添加分项小计，也叫做**margins**。

groupby和顶级方法pandas.pivot_table都能够制作透视表

~~~python
tips = pd.read_csv('examples/tips.csv')
# Add tip percentage of total bill
tips['tip_pct'] = tips['tip'] / tips['total_bill']
tips.pivot_table(index=['day', 'smoker'])

#下面代码含义：行为time和day，列为smoker，以上述的行和列进行分组，比较这些分组的tip_pct和size的大小区别。
tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'],
                 columns='smoker')

#下面的代码传入margins的作用，不统计烟民、非烟民直接是分项小计
tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'],
                 columns='smoker', margins=True)

#要使⽤其他的聚合函数，将其传给aggfunc即可。例如，使⽤count或len可以得到有关分组⼤⼩的交叉表（计数或频率）
tips.pivot_table('tip_pct', index=['time', 'smoker'], columns='day',
                 aggfunc=len, margins=True)
#类似操作，在上一步的基础上填充NA值为0
tips.pivot_table('tip_pct', index=['time', 'size', 'smoker'],
                 columns='day', aggfunc='mean', fill_value=0)
~~~

:spider_web:上述的pivot_table中的选项基本上都用到values、index、columns、aggfunc、fill_value、dropna和margins。

#### 交叉表（Cross-Tabulations: Crosstab）

交叉表（cross-tabulation，简称crosstab）是⼀种⽤于**计算分组频率**的特殊透视表。

小任务：作为调查分析的⼀部分，我们可能想要根据国籍和用手习惯对这段数据进⾏统计汇总（思考可用pivot_table，其天下无敌）。

crosstab的前两个参数可以是**数组或Series，或是数组列表**。

~~~python
from io import StringIO
data = """\
Sample  Nationality  Handedness
1   USA  Right-handed
2   Japan    Left-handed
3   USA  Right-handed
4   Japan    Right-handed
5   Japan    Left-handed
6   Japan    Right-handed
7   USA  Right-handed
8   USA  Left-handed
9   Japan    Right-handed
10  USA  Right-handed"""
data = pd.read_table(StringIO(data), sep='\s+')

#频数统计，前一参数分组依据。后一参数为用手习惯，类型各为series
pd.crosstab(data.Nationality, data.Handedness, margins=True)
#频数统计，类型为数组列表和series
pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)

~~~

## 10.5总结

掌握pandas数据分组⼯具既有助于数据清理，也有助于建模或统计分析⼯作。